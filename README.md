### Spark & Hadoop基础<br>
(必读)[Spark官方文档](http://spark.apache.org/docs/latest/)<br>
(必读)[Spark ML 官方文档](http://spark.apache.org/docs/latest/ml-guide.html)<br>
(必读)[Spark参数配置官方文档](http://spark.apache.org/docs/latest/configuration.html)<br>
[Spark在美团的实践](http://tech.meituan.com/spark-in-meituan.html)<br>
[Spark核心技术与实践](https://yq.aliyun.com/topic/69?utm_content=m_17543)<br>
[spark机器学习算法研究和源码分析](https://github.com/endymecy/spark-ml-source-analysis)<br>
[Apache Spark 内存管理详解](http://wrox.cn/article/100097425/)<br>
[Spark入门实战系列--9.Spark GraphX介绍及实例](http://blog.csdn.net/yirenboy/article/details/47844677)<br>
[HDFS NameNode内存全景](http://tech.meituan.com/namenode.html)<br>
[HDFS NameNode内存详解](http://tech.meituan.com/namenode-memory-detail.html)<br>
[Spark机器学习库（MLlib）指南](http://wrox.cn/article/100077286/)<br>
[大数据学习——Spark Structured Streaming入门编程指南](http://wrox.cn/article/100096772/)<br>
[Databricks孟祥瑞：ALS 在 Spark MLlib 中的实现](http://www.csdn.net/article/2015-05-07/2824641)<br>
(必读)[Spark性能优化指南——基础篇](http://tech.meituan.com/spark-tuning-basic.html)<br>
(必读)[Spark性能优化指南——高级篇](http://tech.meituan.com/spark-tuning-pro.html)<br>
[Spark性能优化之道——解决Spark数据倾斜（Data Skew）的N种姿势](http://www.cnblogs.com/jasongj/p/6508150.html)<br>
[Spark的性能调优](http://www.raychase.net/3546)<br>
[Spark性能优化——和shuffle搏斗](http://www.raychase.net/3788)<br>
[Spark 性能相关参数配置详解－shuffle篇](http://blog.csdn.net/colorant/article/details/38680581)<br>
[Hadoop YARN在异构环境下应用与实践-Hulu大数据架构组负责人董西成](https://pan.baidu.com/s/1hslSG64?errno=0&errmsg=Auth%20Login%20Sucess&&bduss=&ssnerror=0)<br>
[GC调优在Spark应用中的实践](http://dataunion.org/19227.html)<br>
[Spark On YARN内存分配](http://blog.javachen.com/2015/06/09/memory-in-spark-on-yarn.html)<br>
[Apache Spark探秘：实现Map-side Join和Reduce-side Join](http://dongxicheng.org/framework-on-yarn/apache-spark-join-two-tables/)<br>
[spark 应用程序性能优化|12 个优化方法](http://www.36dsj.com/archives/55400)<br>
[Apache Spark Jobs 性能调优（二）](https://www.zybuluo.com/xiaop1987/note/102894)<br>
[Spark Streaming + Elasticsearch构建App异常监控平台](http://tech.meituan.com/spark-streaming-es.html)<br>
[使用基于Apache Spark的随机森林方法预测贷款风险](http://wrox.cn/article/100077656/)<br>
[DataBricks: Apache® Spark™ MLlib 2.x: How to Productionize your Machine Learning Models](https://www.slideshare.net/julesdamji/apache-spark-mllib-2x-how-to-productionize-your-machine-learning-models)<br>
[Using Apache Spark for large-scale language model training](https://code.facebook.com/posts/678403995666478/using-apache-spark-for-large-scale-language-model-training/)<br>
[Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)<br>
[Processing a Trillion Rows Per Second on a Single Machine: How Can Nested Loop Joins be this Fast?](https://databricks.com/blog/2017/02/16/processing-trillion-rows-per-second-single-machine-can-nested-loop-joins-fast.html)<br>
[Recommending items to more than a billion people](https://code.facebook.com/posts/861999383875667/recommending-items-to-more-than-a-billion-people/)<br>
[Apache Spark Scale: A 60 TB+ production use case](https://code.facebook.com/posts/1671373793181703/apache-spark-scale-a-60-tb-production-use-case/)<br>
[Accelerating Apache Spark MLlib with Intel® Math Kernel Library (Intel® MKL)](https://blog.cloudera.com/blog/2017/02/accelerating-apache-spark-mllib-with-intel-math-kernel-library-intel-mkl/)<br>
[Spark Job执行流程源码解析](http://www.jianshu.com/p/ce982840671a)<br>
(实用)[万亿级数据规模下的倾斜调优](https://mp.weixin.qq.com/s/uuSe_kgw5QW_APVtk4zVHg)<br>
[Spark数据本地化-->如何达到性能调优的目的](https://www.cnblogs.com/jxhd1/p/6702224.html?utm_source=itdadao&utm_medium=referral)<br>[Spark算子篇 --Spark算子之aggregateByKey详解](https://www.cnblogs.com/LHWorldBlog/p/8215529.html)



### xgboost<br>
[xgboost原理及应用](http://www.cnblogs.com/zhouxiaohui888/p/6008368.html)<br>
[xgboost入门与实战（原理篇）](http://blog.csdn.net/sb19931201/article/details/52557382)<br>
[XGboost: A Scalable Tree Boosting System论文及源码导读](http://mlnote.com/2016/10/05/a-guide-to-xgboost-A-Scalable-Tree-Boosting-System/)<br>
[XGboost核心源码阅读](http://mlnote.com/2016/10/29/xgboost-code-review-with-paper/)<br>
[Introduction to Boosted Trees](http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)<br>
[Introduction to Boosted Trees](https://xgboost.readthedocs.io/en/latest/model.html)<br>
[XGBoost 与 Boosted Tree(xgboost作者文章)](http://www.52cs.org/?p=429)<br>
[xgboost之spark上运行-scala接口](http://blog.csdn.net/luoyexuge/article/details/71422270)<br>
[xgboost原理](http://blog.csdn.net/a819825294/article/details/51206410)<br>
[Complete Guide to Parameter Tuning in XGBoost (with codes in Python) xgboost参数调优](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)<br>
[大杀器xgboost指南](http://blog.csdn.net/bryan__/article/details/52056112)<br>
[xgboost: 速度快效果好的boosting模型](https://cos.name/2015/03/xgboost/)<br>
<br>

### 随机森林<br>
(必读)[随机森林算法主页](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)<br>
(讲的很清楚)[随机森林](http://www.cnblogs.com/maybe2030/p/4585705.html)<br>
<br>

### word2vec & embeddings<br>
(必读)[word2vec官网](https://code.google.com/archive/p/word2vec/)<br>
[Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)<br>
[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)<br>
[Exploiting Similarities among Languages for Machine Translation](https://arxiv.org/pdf/1309.4168.pdf)<br>
[词向量之Word2vector原理浅析](http://www.jianshu.com/p/b2da4d94a122)<br>
[word2vector学习笔记（一）](http://blog.csdn.net/lingerlanlan/article/details/38048335)<br>
(必读)[Deep Learning实战之word2vec](http://techblog.youdao.com/?p=915)<br>
(必读)[word2vec 中的数学原理详解](https://www.cnblogs.com/peghoty/p/3857839.html)<br>
[word2vec 入门基础](https://www.cnblogs.com/tina-smile/p/5176441.html)<br>
[Deep Learning in NLP （一）词向量和语言模型](http://licstar.net/archives/328)<br>
(必读)[《How to Generate a Good Word Embedding?》导读](http://licstar.net/archives/620)<br>
[机器学习必须熟悉的算法之word2vector（一）](https://www.jianshu.com/p/1405932293ea)<br>
[机器学习必须熟悉的算法之word2vector（二）](https://www.jianshu.com/p/d0e2d00fb4f0)<br>
[机器学习算法实现解析——word2vec源码解析](https://blog.csdn.net/google19890102/article/details/51887344)<br>
[word2vec原理推导与代码分析](http://www.hankcs.com/nlp/word2vec.html)<br>
(必读,CBOW和Skip-gram讲的很通俗易懂)[深度学习笔记——Word2vec和Doc2vec原理理解并结合代码分析](https://blog.csdn.net/mpk_no1/article/details/72458003)<br>
(必读,几种句向量方法介绍)[An Overview of Sentence Embedding Methods](http://mlexplained.com/2017/12/28/an-overview-of-sentence-embedding-methods/)<br>

[cw2vec理论及其实现](https://bamtercelboo.github.io/2018/05/11/cw2vec/)<br>

<br>

### NLP<br>

[推荐系统候选池的两种去重策略](https://mp.weixin.qq.com/s?__biz=MzA4OTk5OTQzMg==&mid=2449231537&idx=1&sn=821697ae129e878b7d5714e4bcd16bc8&chksm=841abf96b36d368065e20e7bf1039ab267efc7067fcbe4b95e854606222735822196de7e2631&mpshare=1&scene=24&srcid=0511BJ2WtN6mAPEfRyWGGGu9&pass_ticket=%2FBT2vdYB2qaBlMLm7WCkhyX%2F%2FCWec2bA4OxLLtm7UXff47YnLRHzb9a4ExG%2BI%2FWk#rd)<br>
[机器学习 | 八大步骤解决90%的NLP问题](https://zhuanlan.zhihu.com/p/36736328)<br>
[基于局部敏感哈希的协同过滤算法之simHash算法](https://www.cnblogs.com/hxsyl/p/4456001.html)<br>
[.NET下文本相似度算法余弦定理和SimHash浅析及应用](http://www.cnblogs.com/weiguang3100/p/4183705.html)<br>
[浅谈simhash及其python实现](https://blog.csdn.net/madujin/article/details/53152619)<br>
[简单易懂讲解simhash算法 hash 哈希](https://blog.csdn.net/le_le_name/article/details/51615931)<br>
[使用SimHash进行海量文本去重](https://www.cnblogs.com/maybe2030/p/5203186.html)<br>
[局部敏感哈希算法(Locality Sensitive Hashing)](http://www.cnblogs.com/maybe2030/p/4953039.html)<br>
[中文文档simhash值计算](https://github.com/yanyiwu/simhash)<br>
[simhash算法原理及实现](https://yanyiwu.com/work/2014/01/30/simhash-shi-xian-xiang-jie.html)<br>
[基于 TWE 模型的关键词提取](http://hejunhao.me/archives/918)<br>
[Topical Word Embeddings](https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9314/9535)<br>
[【论文阅读】Topical Word Embeddings](https://blog.csdn.net/u014568072/article/details/78679925)<br>
[CODE][topical_word_embeddings](https://github.com/largelymfs/topical_word_embeddings)<br>
[深度学习解决NLP问题：语义相似度计算](https://www.cnblogs.com/qniguoym/p/7772561.html)<br>
[语义文本相似度研究进展](https://cloud.tencent.com/developer/news/224166)<br>
(必读)[句子相似度算法比较 Comparing Sentence Similarity Methods](http://nlp.town/blog/sentence-similarity/)<br>
[TF-IDF简介](https://www.cnblogs.com/en-heng/p/5848553.html)<br>

[idf逆文档频率为什么要用log?](http://52opencourse.com/187/idf%E9%80%86%E6%96%87%E6%A1%A3%E9%A2%91%E7%8E%87%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8log)<br>

[第11届INLG会议论文集](https://aclanthology.coli.uni-saarland.de/volumes/proceedings-of-the-11th-international-conference-on-natural-language-generation)

(必读)[自然语言推理-文本蕴含识别简介](https://blog.csdn.net/u010960155/article/details/81335067)

[Textual Entailment Resource Pool](https://aclweb.org/aclwiki/Textual_Entailment_Resource_Pool)

[深度学习在文本分类中的应用](http://blog.csdn.net/u010223750/article/details/51437854)<br>
(必读)[基于深度学习的文本分类？](https://mp.weixin.qq.com/s?__biz=MzIzMjU1NTg3Ng==&mid=2247486702&idx=1&sn=99104c3ea80b0339fcb976deb5918108)<br>
[PyTorch快速入门教程七（RNN做自然语言处理](https://ptorch.com/news/11.html)<br>
[PyTorch快速入门教程八（使用word embedding做自然语言处理的词语预测）](https://ptorch.com/news/12.html)<br>
[PyTorch快速入门教程九（使用LSTM来做判别每个词的词性）](https://ptorch.com/news/13.html)<br>
[Text-CNN 文本分类](http://blog.csdn.net/chuchus/article/details/77847476)<br>
[Convolutional Neural Networks for Sentence Classification](http://arxiv.org/abs/1408.5882)<br>

[Deep Learning for NLP Best Practices(深度学习中NLP的最佳实践)](http://ruder.io/deep-learning-nlp-best-practices/index.html)<br>

(基础讲解)[bert代码](https://daiwk.github.io/posts/nlp-bert-code.html)

(必读)[从Word Embedding到Bert模型——自然语言处理预训练技术发展史](https://mp.weixin.qq.com/s/FHDpx2cYYh9GZsa5nChi4g)

[BERT大火却不懂Transformer？读这一篇就够了](https://zhuanlan.zhihu.com/p/54356280)

[发布一年了，做NLP的还有没看过这篇论文的吗？--“Attention is all you need”](https://blog.csdn.net/sinat_33761963/article/details/83539949)

(必读)[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)<br>

[深度学习中的注意力模型（2017版）](https://zhuanlan.zhihu.com/p/37601161)

(必读)[放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较](https://zhuanlan.zhihu.com/p/54743941)

[完全图解RNN、RNN变体、Seq2Seq、Attention机制](https://zhuanlan.zhihu.com/p/28054589)<br>

[用于文本分类的RNN-Attention网络](http://blog.csdn.net/thriving_fcl/article/details/73381217)<br>

[用深度学习（CNN RNN Attention）解决大规模文本分类问题 - 综述和实践](https://zhuanlan.zhihu.com/p/25928551)<br>

[Text Classification, Part 2 - sentence level Attentional RNN](https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-RNN/)<br>



### LDA<br>

[Latent Dirichlet Allocation原始论文](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)<br>
[LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)<br>
[通俗理解LDA主题模型](http://blog.csdn.net/v_july_v/article/details/41209515?utm_source=tuicool)<br>
[LDA漫游指南](https://yuedu.baidu.com/ebook/d0b441a8ccbff121dd36839a)<br>
(必读)[LDA数学八卦](http://www.52nlp.cn/lda-math-%E6%B1%87%E6%80%BB-lda%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6)<br>
[LDA工程实践之算法篇-1.算法实现正确性验证](http://www.flickering.cn/nlp/2014/07/lda%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E4%B9%8B%E7%AE%97%E6%B3%95%E7%AF%87-1%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E6%AD%A3%E7%A1%AE%E6%80%A7%E9%AA%8C%E8%AF%81/)<br>
[LDA工程实践之算法篇-2.SparseLDA算法](http://www.flickering.cn/nlp/2014/10/lda%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%E4%B9%8B%E7%AE%97%E6%B3%95%E7%AF%87-2-sparselda%E7%AE%97%E6%B3%95/)
[主题模型算法](http://blog.sina.com.cn/s/blog_8eee7fb60101czhx.html)<br>
[Spark LDA文档](https://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda)<br>
<br>

### SVD<br>
[We Recommend a Singular Value Decomposition](http://www.ams.org/samplings/feature-column/fcarc-svd)<br>
[奇异值分解](http://www.flickering.cn/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/2015/01/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%EF%BC%88we-recommend-a-singular-value-decomposition%EF%BC%89/)<br>
<br>

### Machine Learing & Deep Learning<br>
[浅谈机器学习基础（上）](http://www.jianshu.com/p/ed9ae5385b89)<br>
(入门必读，讲清了很多基础概念)[零基础入门深度学习](https://www.zybuluo.com/hanbingtao/note/433855)<br>
(必读)[系列教程：动手学深度学习-使用MXNet](http://zh.gluon.ai/)<br>
(必读)[系列教程：机器学习速成课程-使用TensorFlow(google出品)](https://developers.google.cn/machine-learning/crash-course/)<br>
(必读)[pytorch官方教程:一系列入门示例](http://pytorch.org/tutorials/index.html)<br>
(必读)[pytorch官方教程:使用RNN进行名字分类](http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#sphx-glr-intermediate-char-rnn-classification-tutorial-py)<br>
[pytorch官方教程配套源码](https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification)<br>
[莫烦的机器学习专栏：机器学习本来可以很简单](https://zhuanlan.zhihu.com/morvan)<br>
[深度炼丹专栏:分享在深度学习的一些项目实践与经验](https://zhuanlan.zhihu.com/c_94953554)<br>
(入门必读)[莫烦的pytorch视频教程](https://morvanzhou.github.io/tutorials/machine-learning/torch/)<br>
[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)<br>
[[译] 理解 LSTM 网络](https://www.jianshu.com/p/9dc9f41f0b29)<br>
[人工智障的深度瞎学之路:知乎看山杯夺冠记](https://zhuanlan.zhihu.com/p/28923961)<br>
[基于pytorch的CNN、LSTM神经网络模型调参小结](https://www.cnblogs.com/bamtercelboo/p/7469005.html)<br>
[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)<br>
[序列模型和基于LSTM的循环神经网络](https://zhuanlan.zhihu.com/p/28448135)<br>
[卷积神经网络(CNN)防止过拟合的方法](http://blog.csdn.net/leo_xu06/article/details/71320727)<br>
[训练集样本不平衡问题对CNN的影响](https://zhuanlan.zhihu.com/p/23444244)<br>

[pytorch loss function 总结](https://blog.csdn.net/zhangxb35/article/details/72464152?utm_source=itdadao&utm_medium=referral)

[关于Batch Normalization的另一种理解](https://blog.csdn.net/aichipmunk/article/details/54234646)

[卷积神经网络（CNN）之一维卷积、二维卷积、三维卷积详解](https://www.cnblogs.com/szxspark/p/8445406.html)

[浅析深度ResNet有效的原理](https://blog.csdn.net/u014296502/article/details/80438616)

[LSTM参数个数计算](https://blog.csdn.net/taoyafan/article/details/82803943)

[LSTM的参数数量如何计算？](https://www.zhihu.com/question/263700757)

[LSTM的神经元个数](https://www.cnblogs.com/wushaogui/p/9176617.html)

[LSTM 优化之路](https://mp.weixin.qq.com/s/H0AKuKn1XQDEa06jDm3Nug)

[如何为你的回归问题选择最合适的机器学习方法？](https://mp.weixin.qq.com/s?src=11&timestamp=1555468952&ver=1551&signature=P0oWM7PiddzJ82tv2m4Fdc65p8aLwiLwfXGZDMDmrawrtzoJkeGkC4-ntPqDohAM1TzbJmofMfu1pnLGtRRPWZqoTYsJ1Xd2MNLI*JFXWTRZ4J5QprWjwj*KpSALkM3p&new=1)

<br>

### 聚类<br>
(必读－讲的很清楚)[机器学习sklearn19.0聚类算法——Kmeans算法](https://blog.csdn.net/loveliuzz/article/details/78783773)<br>
[sklearn keans算法官方文档](http://scikit-learn.org/stable/modules/clustering.html#k-means)<br>
[K-means与K-means++](https://www.cnblogs.com/wang2825/articles/8696830.html)<br>
[当我们在谈论K-means：总结](https://zhuanlan.zhihu.com/p/25032944)<br>
[K-means聚类算法的三种改进(K-means++,ISODATA,Kernel K-means)介绍与对比](http://www.cnblogs.com/yixuan-xu/p/6272208.html)<br>
<br>

### 数学<br>
(必读)[极大似然估计详解](https://blog.csdn.net/zengxiantao1994/article/details/72787849)<br>
[L1正则和L2正则的比较分析详解](https://blog.csdn.net/w5688414/article/details/78046960)<br>
(必读)[机器学习中的范数规则化之（一）L0、L1与L2范数](https://blog.csdn.net/zouxy09/article/details/24971995)<br>
[机器学习中的范数规则化之（二）核范数与规则项参数选择](https://blog.csdn.net/zouxy09/article/details/24972869)<br>
(必读)[机器学习中正则化项L1和L2的直观理解](https://blog.csdn.net/jinping_shi/article/details/52433975)<br>
[更好地理解正则化：可视化模型权重分布](https://community.bigquant.com/t/%E6%9B%B4%E5%A5%BD%E5%9C%B0%E7%90%86%E8%A7%A3%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E5%88%86%E5%B8%83/113188)<br>

[HMM与分词、词性标注、命名实体识别](http://www.hankcs.com/nlp/hmm-and-segmentation-tagging-named-entity-recognition.html)

[如何用简单易懂的例子解释隐马尔可夫模型？](https://www.zhihu.com/question/20962240)

[Johns Hopkins大学Jason Eisner教授的HMM教程](http://www.cs.jhu.edu/~jason/papers/#eisner-2002-tnlp)(可互动的excel示例，非常直观)

[一文搞懂HMM（隐马尔可夫模型）](https://www.cnblogs.com/skyme/p/4651331.html)

[Dynamics of optimizing Gaussian mixture models](http://www.cs.jhu.edu/~jason/tutorials/GMM-optimization.html)

<br>

### 其他<br>
[Kaggle 首战拿银总结 | 入门指导 (长文、干货）](https://jizhi.im/blog/post/kaggle_silver)<br>
[斗鱼大数据的玩法](http://wrox.cn/article/100098261/)<br>
[Storm 的可靠性保证测试](http://tech.meituan.com/test-of-storms-reliability.html)<br>
[Scaling Apache Giraph to a trillion edges](https://code.facebook.com/posts/509727595776839/scaling-apache-giraph-to-a-trillion-edges/)<br>
[Large-scale graph partitioning with Apache Giraph](https://code.facebook.com/posts/274771932683700/large-scale-graph-partitioning-with-apache-giraph/)<br>
[逻辑回归算法](http://blog.nsfocus.net/tech/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/2016/05/19/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95.html)<br>
[阿里AI界的新伙伴，1秒钟自动生成20000条文案](https://yq.aliyun.com/articles/603683?utm_content=m_1000003675)<br>
[阿里-搜索团队智能内容生成实践](https://zhuanlan.zhihu.com/p/33956907)<br>
[为电商而生的知识图谱，如何感应用户需求？](https://mp.weixin.qq.com/s?__biz=MzIzOTU0NTQ0MA==&mid=2247488155&idx=1&sn=a0ec228c32153c48d44bbd2d5ff8d003)<br>
[语音信号处理之（四）梅尔频率倒谱系数（MFCC）](https://blog.csdn.net/zouxy09/article/details/9156785)<br>